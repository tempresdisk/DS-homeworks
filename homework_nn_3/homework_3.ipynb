{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82dbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cd6ee",
   "metadata": {},
   "source": [
    "### 1. Обучить AE собственной архитектуры на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ea083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "num_epoch = 5\n",
    "cuda_device = -1\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2225af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 28*28 -> hidden -> out\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.15)\n",
    "        self.linear3 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation(self.linear1(x)))\n",
    "        x = self.dropout2(self.activation(self.linear2(x)))\n",
    "        x = self.activation(self.linear3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # encoder_out -> hidden -> 28*28\n",
    "    def __init__(self, latent_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.15)\n",
    "        self.linear3 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation(self.linear1(x)))\n",
    "        x = self.dropout2(self.activation(self.linear2(x)))\n",
    "        x = self.activation(self.linear3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassicAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, enc_hidden_dim, dec_hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, enc_hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, dec_hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb2f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "\n",
    "        pics.append(numpy.array(item[0]))\n",
    "        target.append(item[1])\n",
    "    return {\n",
    "        'data': torch.from_numpy(numpy.array(pics)).float() / 255,\n",
    "        'target': torch.from_numpy(numpy.array(target)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf51ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassicAutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (linear1): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (dropout1): Dropout(p=0.15, inplace=False)\n",
       "    (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (dropout2): Dropout(p=0.15, inplace=False)\n",
       "    (linear3): Linear(in_features=200, out_features=32, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear1): Linear(in_features=32, out_features=300, bias=True)\n",
       "    (dropout1): Dropout(p=0.15, inplace=False)\n",
       "    (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (dropout2): Dropout(p=0.15, inplace=False)\n",
       "    (linear3): Linear(in_features=300, out_features=784, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ClassicAutoEncoder(28*28, 200, 300, 32)\n",
    "model.train()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fce9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29edaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "dataset = datasets.MNIST('.', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7140f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0367b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "epoch: 1\n",
      "tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "epoch: 2\n",
      "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "epoch: 3\n",
      "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0341, grad_fn=<MseLossBackward0>)\n",
      "epoch: 4\n",
      "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0317, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#dataloder\n",
    "for epoch in range(num_epoch):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    print(f'epoch: {epoch}')\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        data = batch['data'].to(device).view(batch['data'].size(0), -1)\n",
    "        optim.zero_grad()\n",
    "        predict = model(data)\n",
    "        loss = loss_func(predict, data)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step % 100 == 0):\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee93e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31095\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test = dataset.data[1].view(1,-1).long() / 255\n",
    "    test = test.to(device)\n",
    "    predict = model(test)\n",
    "    test = test[0].view(28, 28).detach().cpu().numpy()\n",
    "    print((test*255).astype(int).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4475caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a0e15",
   "metadata": {},
   "source": [
    "### 2. Обучить VAE собственной архитектуры на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218d1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "cuda_device = -1\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e44f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 28*28 -> hidden -> out\n",
    "    def __init__(self, in_chan, hidden_ch, out_channels):\n",
    "        super().__init__()\n",
    "        #conv2d -> maxpool2d -> conv2d -> maxpool2d -> conv2d\n",
    "        self.conv1 = nn.Conv2d(in_chan, hidden_ch, kernel_size=5, stride=1, padding=2) # 28 x28\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # 14 x 14\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, stride=1, padding=1)  # 14 x 14\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 7 x 7\n",
    "        self.conv_mu = nn.Conv2d(hidden_ch, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_sigma = nn.Conv2d(hidden_ch, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x): # -> 7x7\n",
    "        x = self.activation(self.pool1(self.conv1(x)))\n",
    "        x = self.activation(self.pool2(self.conv2(x)))\n",
    "        mu = self.activation(self.conv_mu(x))\n",
    "        sigma = torch.exp(self.conv_sigma(x))\n",
    "\n",
    "        return mu, sigma\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    #conv2d -> upsampling2d -> conv2d -> upsampling2d -> conv2d\n",
    "    def __init__(self, in_chan, hidden_ch, out_chan):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chan, hidden_ch, kernel_size=3, stride=1, padding=1)  # 7 x 7\n",
    "        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)  # 14 x 14\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, stride=1, padding=1)  # 14 x 14\n",
    "        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=2)  # 28 x 28\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, out_chan, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x): # -> 28 x 28\n",
    "        x = self.activation(self.upsample1(self.conv1(x)))\n",
    "        x = self.activation(self.upsample2(self.conv2(x)))\n",
    "        x = self.activation(self.conv3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VarAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_ch, enc_hidden_ch, dec_hidden_ch, latent_ch):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_ch, enc_hidden_ch, latent_ch)\n",
    "        self.decoder = Decoder(latent_ch, dec_hidden_ch, input_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encoder(x)\n",
    "        x = sampling(mu, sigma)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x, mu, sigma\n",
    "\n",
    "\n",
    "# sampling\n",
    "def sampling(mu, sigma):\n",
    "    return mu + sigma * torch.normal(torch.zeros_like(sigma),\n",
    "                                     torch.ones_like(sigma))\n",
    "\n",
    "\n",
    "def kl_loss(mu, sigma):\n",
    "    p = torch.distributions.Normal(mu, sigma)\n",
    "    q = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(sigma))\n",
    "\n",
    "    return torch.distributions.kl_divergence(p, q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c841395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarAutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv_mu): Conv2d(10, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_sigma): Conv2d(10, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsample1): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "    (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsample2): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "    (conv3): Conv2d(10, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vae = VarAutoEncoder(1, 10, 10, 1)\n",
    "model_vae.train()\n",
    "model_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf7d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optim = torch.optim.Adam(model_vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9575045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e4d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "kl_loss: 0.004905720241367817, criterion_loss: 0.10077963024377823\n",
      "kl_loss: 0.0015488872304558754, criterion_loss: 0.09624964743852615\n",
      "kl_loss: 0.0008056904189288616, criterion_loss: 0.09102003276348114\n",
      "epoch: 1\n",
      "kl_loss: 0.0006759946700185537, criterion_loss: 0.08561217039823532\n",
      "kl_loss: 0.00044096438796259463, criterion_loss: 0.08443830162286758\n",
      "kl_loss: 0.00031492026755586267, criterion_loss: 0.08199407905340195\n",
      "epoch: 2\n",
      "kl_loss: 0.00028456305153667927, criterion_loss: 0.08607540279626846\n",
      "kl_loss: 0.00021831518097314984, criterion_loss: 0.0849464014172554\n",
      "kl_loss: 0.00017418881179764867, criterion_loss: 0.0817333310842514\n",
      "epoch: 3\n",
      "kl_loss: 0.000161705058417283, criterion_loss: 0.08153034746646881\n",
      "kl_loss: 0.00013345308252610266, criterion_loss: 0.08261124044656754\n",
      "kl_loss: 0.0001132567340391688, criterion_loss: 0.07999932020902634\n",
      "epoch: 4\n",
      "kl_loss: 0.00010759594442788512, criterion_loss: 0.08051405102014542\n",
      "kl_loss: 9.385337762068957e-05, criterion_loss: 0.08101602643728256\n",
      "kl_loss: 8.220483141485602e-05, criterion_loss: 0.08054450154304504\n",
      "epoch: 5\n",
      "kl_loss: 7.943939999677241e-05, criterion_loss: 0.08023212105035782\n",
      "kl_loss: 7.20225361874327e-05, criterion_loss: 0.08188167959451675\n",
      "kl_loss: 6.709322769893333e-05, criterion_loss: 0.0802202820777893\n",
      "epoch: 6\n",
      "kl_loss: 6.55849653412588e-05, criterion_loss: 0.07954015582799911\n",
      "kl_loss: 6.420121644623578e-05, criterion_loss: 0.08003202080726624\n",
      "kl_loss: 6.774289795430377e-05, criterion_loss: 0.07892529666423798\n",
      "epoch: 7\n",
      "kl_loss: 7.013030699454248e-05, criterion_loss: 0.08124180883169174\n",
      "kl_loss: 8.970229828264564e-05, criterion_loss: 0.07939871400594711\n",
      "kl_loss: 0.00011195511615369469, criterion_loss: 0.07984253764152527\n",
      "epoch: 8\n",
      "kl_loss: 0.00010961955558741465, criterion_loss: 0.08189497888088226\n",
      "kl_loss: 0.0001467484689783305, criterion_loss: 0.08201369643211365\n",
      "kl_loss: 0.0002297597675351426, criterion_loss: 0.07840172201395035\n",
      "epoch: 9\n",
      "kl_loss: 0.00022351994994096458, criterion_loss: 0.07847187668085098\n",
      "kl_loss: 0.0003180259664077312, criterion_loss: 0.07963424921035767\n",
      "kl_loss: 0.0005198090802878141, criterion_loss: 0.0800367146730423\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    print(f'epoch: {epoch}')\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        data = batch['data'].to(device).unsqueeze(1)\n",
    "        optim.zero_grad()\n",
    "        predict, mu, sigma = model_vae(data)\n",
    "        #loss\n",
    "        kl = kl_loss(mu, sigma)\n",
    "        crit_loss = criterion(data, predict)\n",
    "        loss = 0.1 * kl + crit_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step % 100 == 0):\n",
    "            print(f'kl_loss: {kl.item()}, criterion_loss: {crit_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7809503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33676\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_vae.eval()\n",
    "    test = dataset.data[255].unsqueeze(0).unsqueeze(0).float() / 255\n",
    "    test = test.to(device)\n",
    "    predict = model_vae(test)\n",
    "    test = test[0].view(28, 28).detach().cpu().numpy()\n",
    "    print((test*255).astype(int).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
